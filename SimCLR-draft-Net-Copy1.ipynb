{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6a9f50-fdcc-4f49-9622-8385f1b8be2c",
   "metadata": {},
   "source": [
    "## simclr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedd9f7f-f08f-43bf-b406-dee5e610b4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightly \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lightly import data\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ac96ba-dcfd-45d8-a835-2f7114b27159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6379\n",
    "TEST_NUM_CLASSES = 24\n",
    "num_workers = 0#int(os.cpu_count()//2)\n",
    "seed = 1\n",
    "max_epochs = 10\n",
    "#input_size = 128\n",
    "#num_ftrs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47142a1e-77c3-4a43-870a-4f540e6510dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f841b3-655d-4fc0-9023-2557ebb9172d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_data_train = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/train'\n",
    "path_to_data_test = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test'\n",
    "path_to_data_test1 = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eaf9d05-e1f8-413a-b9b4-0b15462d9f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ElasticTransform(alpha=200.0, sigma=10.0, interpolation=InterpolationMode.BILINEAR, fill=255),\n",
    "    transforms.RandomAffine(degrees=(-20, 20), scale=(0.8, 1.2), fill = 255),\n",
    "    transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])\n",
    "\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ElasticTransform(alpha=200.0, sigma=10.0, interpolation=InterpolationMode.BILINEAR, fill=255),\n",
    "    transforms.RandomAffine(degrees=(-20, 20), scale=(0.8, 1.2), fill = 255),\n",
    "    #transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f235a3-059f-4c09-bb2f-80c574d609d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### データローダー"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381d17b-e118-4585-b66b-aa8f5ada0358",
   "metadata": {},
   "source": [
    "・バッチサイズ（字ごとに異なる）\n",
    "・画足りない情報: ラベルに入れる\n",
    "・バッチサイズの指定を変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7f93bf-327b-4897-b6ab-29000eb367fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_rgba_to_rgb_with_background(image, background=(255, 255, 255)):\n",
    "    \"\"\"RGBA画像を指定した背景色でRGB画像に変換\"\"\"\n",
    "    if image.mode == 'RGBA':\n",
    "        # 背景を指定して新しい画像を作成\n",
    "        background_image = Image.new('RGB', image.size, background)\n",
    "        # アルファチャンネルを使用してマスクを適用\n",
    "        background_image.paste(image, mask=image.split()[3])  # アルファチャンネルでマスク\n",
    "        return background_image\n",
    "    return image  # すでにRGBの場合はそのまま返す\n",
    "\n",
    "def label_data_dict(path_to_data):\n",
    "    d = dict() # 画像に対しラベル\n",
    "    class_indices = dict() # ラベルに対し画像が何個あるか\n",
    "    for idx, path in enumerate(glob.glob(f'{path_to_data}/*/*.png')):\n",
    "        char = path.split('/')[-2]\n",
    "        d[path] = char\n",
    "        \n",
    "        if char not in class_indices:\n",
    "            class_indices[char] = [idx]\n",
    "        else:\n",
    "            class_indices[char] += [idx]\n",
    "    return d, class_indices\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        #self.classes = classes\n",
    "        \n",
    "        data, _ = label_data_dict(path_to_data)\n",
    "        self.image_paths.extend(list(data.keys()))\n",
    "        self.labels.extend(list(data.values()))\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        label_index = self.classes.index(label)\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image = convert_rgba_to_rgb_with_background(image, background=(255, 255, 255))  # 白背景\n",
    "        #image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image, label_index\n",
    "\n",
    "# class_indices: {'label':[data], ...}\n",
    "class ClassBatchSampler:\n",
    "    def __init__(self, class_indices):\n",
    "        self.class_indices = class_indices\n",
    "        self.classes = list(class_indices.keys())\n",
    "        #self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 各クラスのインデックスを順に返す\n",
    "        for class_label in self.classes:\n",
    "            indices = self.class_indices[class_label]\n",
    "            #print(f\"Sampling indices for class {class_label}: {indices}\", flush=True)  # デバッグ出力\n",
    "            yield indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes)\n",
    "        \n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# データセットを作成\n",
    "#dataset = CustomDataset(path_to_data, transform=transform)\n",
    "\n",
    "#_, class_indices = label_data_dict(path_to_data)\n",
    "\n",
    "# サンプラーを使ってデータローダーを作成\n",
    "#sampler = ClassBatchSampler(class_indices)\n",
    "#dataloader = DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "# データローダーでクラスごとにデータを取得\n",
    "#for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "#    print(f\"Batch {batch_idx}:\")\n",
    "#    print(f\"Data: {data}\")\n",
    "#    print(f\"Labels: {labels}\")\n",
    "#    print(f\"Batch size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4bf53c0-d009-47da-b0d9-17d874145f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データセットを作成\n",
    "dataset_train = CustomDataset(path_to_data_train, transform=transform_train)\n",
    "dataset_test = CustomDataset(path_to_data_test, transform=transform_test)\n",
    "\n",
    "_, class_indices_train = label_data_dict(path_to_data_train)\n",
    "_, class_indices_test = label_data_dict(path_to_data_test)\n",
    "\n",
    "# サンプラーを使って訓練データローダーを作成\n",
    "sampler = ClassBatchSampler(class_indices_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_sampler=sampler, num_workers=num_workers)\n",
    "\n",
    "# サンプラーを使ってテストデータローダーを作成\n",
    "sampler = ClassBatchSampler(class_indices_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894ae26-c343-465c-a067-1d2d5923e6ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38468cd3-8214-4ee5-bf97-e48476cff746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization of Conv2D parameters according to He et al. (2015)\n",
    "def he_init(conv2d_layer):\n",
    "    # kernel size: k1 x k2\n",
    "    k1, k2 = conv2d_layer.kernel_size\n",
    "\n",
    "    # input channel\n",
    "    c = conv2d_layer.in_channels\n",
    "\n",
    "    # number of summands\n",
    "    n = k1 * k2 * c\n",
    "    \n",
    "    # good standard deviation : sqrt(2/n)\n",
    "    std = (2 / n) ** 0.5\n",
    "\n",
    "    # init kernel params ~ Normal(0, std^2)\n",
    "    nn.init.normal_(conv2d_layer.weight, mean=0.0, std=std)\n",
    "\n",
    "    # init bias = 0\n",
    "    nn.init.zeros_(conv2d_layer.bias)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,num_classes=NUM_CLASSES):\n",
    "        super(Net, self).__init__()\n",
    "        # mtzk: p=0.005 is too small\n",
    "        #self.embedding_dropout = nn.Dropout(p = 0.005)\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        #self.conv1 = nn.Conv2d(3,64,3)\n",
    "        self.conv1 = nn.Conv2d(1,64,3) #single-channel\n",
    "        # nn.init.normal_(self.conv1.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64,128,3)\n",
    "        # nn.init.normal_(self.conv2.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128,512,3)\n",
    "        # nn.init.normal_(self.conv3.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(512,512,3)\n",
    "        # nn.init.normal_(self.conv4.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv4)\n",
    "\n",
    "        # mtzk: BatchNorm2d は学習パラメータがあるので層ごとに違うのを使うべき\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 5 * 5, 4096)\n",
    "        # mtzk: - std changed according to He et al. (2015)\n",
    "        #       - init bias = zero\n",
    "        # nn.init.normal_(self.fc1.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        self.bn_fc1 = nn.BatchNorm1d(num_features=4096)\n",
    "\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        # nn.init.normal_(self.fc2.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        self.bn_fc2 = nn.BatchNorm1d(num_features=4096)\n",
    "\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "        # nn.init.normal_(self.fc3.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        # mtzk: BatchNorm2d は学習パラメータがあるので層ごとに違うのを使うべき\n",
    "        # x = self.bn3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4529c2cd-0d32-4841-88fd-2fd20035f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "\n",
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self, batch_size=10, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size \n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        net = Net()\n",
    "        self.backbone = Net(num_classes=NUM_CLASSES)\n",
    "        \n",
    "        hidden_dim = NUM_CLASSES#net.fc1.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=2048,\n",
    "            output_dim=128,\n",
    "            num_layers=2,\n",
    "            batch_norm=True\n",
    "        )\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        #print(f\"Flattened output size: {h.size()}\")\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "#    def training_step(self, batch, batch_idx):\n",
    "        #print(f\"Batch content: {batch}\")  # バッチの内容を出力して確認\n",
    "#        (x0, x1), *_ = batch\n",
    "#        z0 = self.forward(x0)\n",
    "#        z1 = self.forward(x1)\n",
    "#        loss = self.criterion(z0, z1)\n",
    "#        self.log(\"train_loss_ssl\", loss)\n",
    "#        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch  # 画像とラベルを分けて取得\n",
    "\n",
    "        # 複数個の異なるビューを作成\n",
    "        views = [self.transform(images) for _ in range(5)]\n",
    "\n",
    "        # 各ビューについて順伝播を行い、特徴量を計算\n",
    "        embeddings = [self.forward(view) for view in views]\n",
    "\n",
    "        # 損失を計算（例: 各ペアの特徴量間で損失を計算し、その平均をとる）\n",
    "        total_loss = 0\n",
    "        num_pairs = 0\n",
    "\n",
    "        # すべてのペアの組み合わせで損失を計算\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i + 1, len(embeddings)):\n",
    "                total_loss += self.criterion(embeddings[i], embeddings[j])\n",
    "                num_pairs += 1\n",
    "\n",
    "        # ペア間の平均損失を計算\n",
    "        loss = total_loss / num_pairs\n",
    "\n",
    "        # ログに損失を記録\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):# lr=0.075*(self.batch_size)**(1/2)\n",
    "        #optim = torch.optim.SGD(\n",
    "        #    self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        #)\n",
    "        optim = torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-3, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, max_epochs\n",
    "        )\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8ceff2-6f23-47c2-b339-725be1506a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "\n",
    "class SimCLRClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, batch_size=10, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # SimCLR のバックボーンを定義\n",
    "        net = Net()\n",
    "        self.backbone = Net(num_classes=NUM_CLASSES)\n",
    "        \n",
    "        # Projection Head の定義\n",
    "        hidden_dim = NUM_CLASSES#net.fc1.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=2048,\n",
    "            output_dim=128,\n",
    "            num_layers=2,\n",
    "            batch_norm=True\n",
    "        )\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # 分類層の定義\n",
    "        self.classifier = torch.nn.Linear(128, TEST_NUM_CLASSES)\n",
    "        \n",
    "        # 損失関数を定義\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        #print(f\"Flattened output size: {h.size()}\")\n",
    "        projections = self.projection_head(h)\n",
    "        \n",
    "        # 分類層でクラスごとのスコアを出力\n",
    "        logits = self.classifier(projections)  # classifier も必ず使用\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch  # 画像とラベルを分けて取得\n",
    "\n",
    "        # 複数個の異なるビューを作成\n",
    "        views = [self.transform(images) for _ in range(5)]\n",
    "\n",
    "        # 各ビューについて順伝播を行い、特徴量を計算\n",
    "        embeddings = [self.forward(view) for view in views]\n",
    "\n",
    "        # 損失を計算（例: 各ペアの特徴量間で損失を計算し、その平均をとる）\n",
    "        total_loss = 0\n",
    "        num_pairs = 0\n",
    "\n",
    "        # すべてのペアの組み合わせで損失を計算\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i + 1, len(embeddings)):\n",
    "                total_loss += self.criterion(embeddings[i], embeddings[j])\n",
    "                num_pairs += 1\n",
    "\n",
    "        # ペア間の平均損失を計算\n",
    "        loss = total_loss / num_pairs\n",
    "\n",
    "        # ログに損失を記録\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "#    def validation_step(self, batch, batch_idx):\n",
    "#        images, labels = batch\n",
    "        \n",
    "        # 順伝播で出力を計算\n",
    "#        logits = self.forward(images)\n",
    "        \n",
    "        # 損失と精度を計算\n",
    "#        loss = self.criterion(logits, labels)\n",
    "#        acc = (logits.argmax(1) == labels).float().mean()\n",
    "        \n",
    "        # ログに記録\n",
    "#        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "#        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):# lr=0.075*(self.batch_size)**(1/2)\n",
    "        #optim = torch.optim.SGD(\n",
    "        #    self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        #)\n",
    "        optim = torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-3, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, max_epochs\n",
    "        )\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af53798-54e7-441b-b15d-e8597894e051",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b602417-861a-43f5-8244-089a19a83f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # GPUが使用可能かどうかを確認\n",
    "#print(torch.cuda.device_count())  # 使用可能なGPUの数\n",
    "#print(torch.cuda.get_device_name(0))  # GPUの名前\n",
    "accelerator='gpu' if torch.cuda.is_available() else 'cpu'\n",
    "accelerator\n",
    "devices=2 if torch.cuda.is_available() else 1\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438f28ee-9d3b-4e04-80fd-e9d3eaa5097b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabababamb1\u001b[0m (\u001b[33mabababamb1-tokyo-university-of-science\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278cf167-c54b-4e23-8d59-089e472c669b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_dir = \"/data2/abababam1/HandwrittenTextAlign/simclr/checkpoints\"\n",
    "# チェックポイント保存用のコールバック設定\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,  # 保存先ディレクトリ\n",
    "    filename=\"epoch{epoch}-step{step}\",  # ファイル名のフォーマット\n",
    "    save_top_k=1,  # 最新のみ保存\n",
    "    every_n_epochs=2  # 毎エポック保存\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049e987-63be-4050-865f-7c1bd828cfbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 3 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241206_152129-5ykr3od3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs/runs/5ykr3od3' target=\"_blank\">deft-bush-51</a></strong> to <a href='https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs' target=\"_blank\">https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs/runs/5ykr3od3' target=\"_blank\">https://wandb.ai/abababamb1-tokyo-university-of-science/lightning_logs/runs/5ykr3od3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/abababam1/.conda/envs/ImageRecognition/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /data2/abababam1/HandwrittenTextAlign/simclr/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Net                  | 98.4 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 13.3 M | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "111 M     Trainable params\n",
      "0         Non-trainable params\n",
      "111 M     Total params\n",
      "446.892   Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/data/abababam1/.conda/envs/ImageRecognition/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 6158/6379 [43:57<01:34,  2.34it/s, v_num=3od3]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#gpus = [1] if torch.cuda.is_available() else 0\n",
    "\n",
    "wandb_logger = WandbLogger(log_model=\"all\")\n",
    "\n",
    "model = SimCLRModel(batch_size=10, transform=transform_simclr)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=[0,1,2] if torch.cuda.is_available() else 1,  # GPUが使える場合は[2]、使えない場合は1（CPUコア数）\n",
    "    strategy=\"ddp_notebook\",  # データ並列 (DataParallel)\n",
    "    enable_progress_bar=True, # 進捗バーを有効化\n",
    "    log_every_n_steps=100,  # ログの更新間隔を設定\n",
    "    logger=wandb_logger,  # ログ機能を無効化\n",
    "    use_distributed_sampler=False,  # 分散サンプラーを無効化\n",
    "    accumulate_grad_batches=30,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80984ef3-7893-46ea-a3f2-35b0e06d87aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデルの状態を保存\n",
    "torch.save(model.state_dict(), './params/1206-myNet-aug5-10ep.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341ee6f-fd52-4a10-9f83-61fb03fe3584",
   "metadata": {},
   "source": [
    "#### テストデータの埋め込み作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7008a57d-d214-4ddf-a197-cb7d0f2d33f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0012885 ,  0.00160888, -0.00455913, ...,  0.00072197,\n",
       "        -0.00516354, -0.01171149],\n",
       "       [ 0.00126025,  0.00179003, -0.00462212, ...,  0.00090455,\n",
       "        -0.00522257, -0.01198926],\n",
       "       [ 0.00126025,  0.00179003, -0.00462212, ...,  0.00090455,\n",
       "        -0.00522257, -0.01198926],\n",
       "       ...,\n",
       "       [ 0.00126025,  0.00179003, -0.00462212, ...,  0.00090455,\n",
       "        -0.00522257, -0.01198926],\n",
       "       [ 0.00126025,  0.00179003, -0.00462212, ...,  0.00090455,\n",
       "        -0.00522257, -0.01198926],\n",
       "       [ 0.00126025,  0.00179003, -0.00462212, ...,  0.00090455,\n",
       "        -0.00522257, -0.01198926]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_embeddings(model, dataloader):\n",
    "    \"\"\"Generates representations for all images in the dataloader with\n",
    "    the given model\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        #for img, label, fnames in dataloader:\n",
    "        for img, label in dataloader:\n",
    "            img = img.to(model.device)\n",
    "            emb = model.backbone(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "            #filenames.extend(fnames)\n",
    "\n",
    "    if embeddings:  # embeddingsが空でないことを確認\n",
    "        embeddings = torch.cat(embeddings, 0)\n",
    "        embeddings = normalize(embeddings)\n",
    "        return embeddings, filenames\n",
    "    else:\n",
    "        raise RuntimeError(\"No embeddings generated. Please check your model and dataloader.\")\n",
    "\n",
    "model.eval()\n",
    "embeddings, filenames = generate_embeddings(model, dataloader_test)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3149294e-f3f0-4e50-a797-2591f2500ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195413/1190602404.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:01<00:00, 18.14it/s]Test Accuracy: 0.0417, Test Loss: 3.7116\n",
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:01<00:00, 18.08it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      avg_test_acc          0.0416666679084301\n",
      "      avg_test_loss          3.711585283279419\n",
      "        test_acc            0.04780876636505127\n",
      "        test_loss           3.6389455795288086\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 3.6389455795288086,\n",
       "  'test_acc': 0.04780876636505127,\n",
       "  'avg_test_loss': 3.711585283279419,\n",
       "  'avg_test_acc': 0.0416666679084301}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimCLR_Test(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes=TEST_NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.mlp = torch.nn.Linear(128, num_classes)  # SimCLRの出力次元に応じて調整\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        \n",
    "        # テスト結果を保持するリストを初期化\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 特徴抽出してからMLP層でクラス分類\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        logits = self.mlp(features)\n",
    "        return logits\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = self.loss_fn(z, y)\n",
    "\n",
    "        predicted = z.argmax(1)\n",
    "        acc = (predicted == y).float().mean()\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # 損失と精度をリストに保存\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_accuracies.append(acc)\n",
    "        \n",
    "        return {\"test_loss\": loss, \"test_acc\": acc}     \n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # テストデータ全体の平均損失と精度を集計\n",
    "        if self.test_losses and self.test_accuracies:\n",
    "            avg_loss = torch.stack(self.test_losses).mean()\n",
    "            avg_acc = torch.stack(self.test_accuracies).mean()\n",
    "            self.log('avg_test_loss', avg_loss, prog_bar=True)\n",
    "            self.log('avg_test_acc', avg_acc, prog_bar=True)\n",
    "            print(f\"Test Accuracy: {avg_acc:.4f}, Test Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"No test data was processed\")\n",
    "\n",
    "        \n",
    "# .pthファイルから事前学習済みモデルの重みをロード\n",
    "def load_simclr_model(model_path):\n",
    "    model = SimCLRModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # 評価モードに設定\n",
    "    return model\n",
    "\n",
    "#simclr_model = load_simclr_model('./params/1112-myNet-transform-30ep-100.pth')\n",
    "simclr_model = load_simclr_model('./params/1119-myNet-aug5-10ep.pth')\n",
    "\n",
    "# テスト用の評価クラスを初期化\n",
    "test_model = SimCLR_Test(model=simclr_model, num_classes=TEST_NUM_CLASSES)\n",
    "\n",
    "# テストデータローダーの用意\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# PyTorch LightningのTrainerを使用してテストを実行\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1)  # CPUのみを使用\n",
    "trainer.test(test_model, dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b800ecb-ebac-4d4d-a0b6-b30c8698ae7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195413/3197135046.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 6379/6379 [04:38<00:00, 22.88it/s]Test Accuracy: 0.0002, Test Loss: 9.7780\n",
      "Testing DataLoader 0: 100%|██████████| 6379/6379 [04:38<00:00, 22.88it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      avg_test_acc        0.00015676437760703266\n",
      "      avg_test_loss          9.778013229370117\n",
      "        test_acc          0.00010572066094027832\n",
      "        test_loss            9.775991439819336\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 9.775991439819336,\n",
       "  'test_acc': 0.00010572066094027832,\n",
       "  'avg_test_loss': 9.778013229370117,\n",
       "  'avg_test_acc': 0.00015676437760703266}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimCLR_Test(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.mlp = torch.nn.Linear(128, num_classes)  # SimCLRの出力次元に応じて調整\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        \n",
    "        # テスト結果を保持するリストを初期化\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 特徴抽出してからMLP層でクラス分類\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        logits = self.mlp(features)\n",
    "        return logits\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.forward(x)\n",
    "        loss = self.loss_fn(z, y)\n",
    "\n",
    "        predicted = z.argmax(1)\n",
    "        acc = (predicted == y).float().mean()\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # 損失と精度をリストに保存\n",
    "        self.test_losses.append(loss)\n",
    "        self.test_accuracies.append(acc)\n",
    "        \n",
    "        return {\"test_loss\": loss, \"test_acc\": acc}     \n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # テストデータ全体の平均損失と精度を集計\n",
    "        if self.test_losses and self.test_accuracies:\n",
    "            avg_loss = torch.stack(self.test_losses).mean()\n",
    "            avg_acc = torch.stack(self.test_accuracies).mean()\n",
    "            self.log('avg_test_loss', avg_loss, prog_bar=True)\n",
    "            self.log('avg_test_acc', avg_acc, prog_bar=True)\n",
    "            print(f\"Test Accuracy: {avg_acc:.4f}, Test Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"No test data was processed\")\n",
    "\n",
    "        \n",
    "# .pthファイルから事前学習済みモデルの重みをロード\n",
    "def load_simclr_model(model_path):\n",
    "    model = SimCLRModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # 評価モードに設定\n",
    "    return model\n",
    "\n",
    "#simclr_model = load_simclr_model('./params/1112-myNet-transform-30ep-100.pth')\n",
    "simclr_model = load_simclr_model('./params/1112-myNet-aug5-10ep.pth')\n",
    "\n",
    "# テスト用の評価クラスを初期化\n",
    "test_model = SimCLR_Test(model=simclr_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "# PyTorch LightningのTrainerを使用してテストを実行\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1)  # CPUのみを使用\n",
    "trainer.test(test_model, dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b9808c5-76b5-4538-8942-bad514360cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def delete_non_matching_files(base_dir, pattern='*-0.png'):\n",
    "    \"\"\"\n",
    "    指定したディレクトリ以下のすべてのサブディレクトリ内で、\n",
    "    名前に指定されたパターンが含まれないファイルを削除する。\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir (str): 探索する基準となるディレクトリ\n",
    "    - pattern (str): 残したいファイル名のパターン（デフォルトは '*-0.png'）\n",
    "    \"\"\"\n",
    "    # 再帰的にすべてのファイルを探索\n",
    "    all_files = glob.glob(os.path.join(base_dir, '**', '*.png'), recursive=True)\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        # ファイル名に '-0.png' が含まれていなければ削除\n",
    "        if not file_path.endswith('-0.png'):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"削除しました: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"削除に失敗しました: {file_path}, エラー: {e}\")\n",
    "\n",
    "# 実行\n",
    "base_dir = './HandwrittenTextAlign/PRMU/simclr/data/test1'\n",
    "delete_non_matching_files(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede2a7b-ec9b-4855-8682-e942ba994580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_files(src_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    src_dir: 元のディレクトリ\n",
    "    dest_dir: 新しいディレクトリ\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)  # 新しいディレクトリを作成\n",
    "\n",
    "    # 再帰的に src_dir を探索\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # .png ファイルを処理\n",
    "            if file.endswith(\".png\"):\n",
    "                # 元ディレクトリの相対パスを保持\n",
    "                relative_path = os.path.relpath(root, src_dir)\n",
    "                \n",
    "                # 新しいサブディレクトリを作成\n",
    "                subdir = os.path.join(dest_dir, relative_path)\n",
    "                if not os.path.exists(subdir):\n",
    "                    os.makedirs(subdir)\n",
    "                \n",
    "                # ファイル名から語尾（-0, -3 など）を取得\n",
    "                suffix = file.split('-')[-1].split('.')[0]\n",
    "                subsubdir = os.path.join(subdir, f'{subdir}-{suffix}')  # 新しい subsubdir を作成\n",
    "                \n",
    "                if not os.path.exists(subsubdir):\n",
    "                    os.makedirs(subsubdir)\n",
    "                \n",
    "                # 新しいファイルパスを生成\n",
    "                new_file_path = os.path.join(subsubdir, file)\n",
    "                \n",
    "                # ファイルをコピー\n",
    "                shutil.copy(file_path, new_file_path)\n",
    "                print(f\"コピーしました: {file_path} -> {new_file_path}\")\n",
    "\n",
    "# 実行\n",
    "src_dir = \"./HandwrittenTextAlign/PRMU/simclr/data/test\"  # 元のディレクトリ\n",
    "dest_dir = \"./HandwrittenTextAlign/PRMU/simclr/data/test2\"  # 新しいディレクトリ\n",
    "process_files(src_dir, dest_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageRecognition",
   "language": "python",
   "name": "imagerecognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
