{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de42e1-21ba-4632-bef8-5c26a3af5a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightly \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lightly import data\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66f19c-f0fe-40ec-a80c-8836aa17f8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6379\n",
    "TEST_NUM_CLASSES = 24\n",
    "num_workers = 0#int(os.cpu_count()//2)\n",
    "seed = 1\n",
    "max_epochs = 10\n",
    "#input_size = 128\n",
    "#num_ftrs = 32\n",
    "\n",
    "path_to_data_train = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/train'\n",
    "path_to_data_test = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test'\n",
    "#path_to_data_test = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/E_class'\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ElasticTransform(alpha=200.0, sigma=10.0, interpolation=InterpolationMode.BILINEAR, fill=255),\n",
    "    transforms.RandomAffine(degrees=(-20, 20), scale=(0.8, 1.2), fill = 255),\n",
    "    transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])\n",
    "\n",
    "transform_simclr = transforms.Compose([\n",
    "    transforms.Resize((64, 63),antialias=True),  # 画像のサイズ変更\n",
    "    transforms.Grayscale(num_output_channels=1), #single-channel\n",
    "    transforms.ElasticTransform(alpha=200.0, sigma=10.0, interpolation=InterpolationMode.BILINEAR, fill=255),\n",
    "    transforms.RandomAffine(degrees=(-20, 20), scale=(0.8, 1.2), fill = 255),\n",
    "    #transforms.ToTensor(),           # テンソルに変換\n",
    "    transforms.Normalize((0.5,), (0.5,)) #single-channel normalization\n",
    "])\n",
    "\n",
    "\n",
    "def convert_rgba_to_rgb_with_background(image, background=(255, 255, 255)):\n",
    "    \"\"\"RGBA画像を指定した背景色でRGB画像に変換\"\"\"\n",
    "    if image.mode == 'RGBA':\n",
    "        # 背景を指定して新しい画像を作成\n",
    "        background_image = Image.new('RGB', image.size, background)\n",
    "        # アルファチャンネルを使用してマスクを適用\n",
    "        background_image.paste(image, mask=image.split()[3])  # アルファチャンネルでマスク\n",
    "        return background_image\n",
    "    return image  # すでにRGBの場合はそのまま返す\n",
    "\n",
    "\n",
    "def label_data_dict(path_to_data):\n",
    "    d = dict() # 画像に対しラベル\n",
    "    class_indices = dict() # ラベルに対し画像が何個あるか\n",
    "    for idx, path in enumerate(glob.glob(f'{path_to_data}/*/*.png')):\n",
    "        char = path.split('/')[-2]\n",
    "        d[path] = char\n",
    "        \n",
    "        if char not in class_indices:\n",
    "            class_indices[char] = [idx]\n",
    "        else:\n",
    "            class_indices[char] += [idx]\n",
    "    return d, class_indices\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        #self.classes = classes\n",
    "        \n",
    "        data, _ = label_data_dict(path_to_data)\n",
    "        self.image_paths.extend(list(data.keys()))\n",
    "        self.labels.extend(list(data.values()))\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        label_index = self.classes.index(label)\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image = convert_rgba_to_rgb_with_background(image, background=(255, 255, 255))  # 白背景\n",
    "        #image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image, label_index\n",
    "\n",
    "# class_indices: {'label':[data], ...}\n",
    "class ClassBatchSampler:\n",
    "    def __init__(self, class_indices):\n",
    "        self.class_indices = class_indices\n",
    "        self.classes = list(class_indices.keys())\n",
    "        #self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 各クラスのインデックスを順に返す\n",
    "        for class_label in self.classes:\n",
    "            indices = self.class_indices[class_label]\n",
    "            #print(f\"Sampling indices for class {class_label}: {indices}\", flush=True)  # デバッグ出力\n",
    "            yield indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes)\n",
    "\n",
    "# データセットを作成\n",
    "dataset_train = CustomDataset(path_to_data_train, transform=transform_train)\n",
    "dataset_test = CustomDataset(path_to_data_test, transform=transform_test)\n",
    "\n",
    "_, class_indices_train = label_data_dict(path_to_data_train)\n",
    "_, class_indices_test = label_data_dict(path_to_data_test)\n",
    "\n",
    "# サンプラーを使って訓練データローダーを作成\n",
    "sampler = ClassBatchSampler(class_indices_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_sampler=sampler, num_workers=num_workers)\n",
    "\n",
    "# サンプラーを使ってテストデータローダーを作成\n",
    "sampler = ClassBatchSampler(class_indices_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a4fb5-8c06-4259-8592-b404130f59d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization of Conv2D parameters according to He et al. (2015)\n",
    "def he_init(conv2d_layer):\n",
    "    # kernel size: k1 x k2\n",
    "    k1, k2 = conv2d_layer.kernel_size\n",
    "\n",
    "    # input channel\n",
    "    c = conv2d_layer.in_channels\n",
    "\n",
    "    # number of summands\n",
    "    n = k1 * k2 * c\n",
    "    \n",
    "    # good standard deviation : sqrt(2/n)\n",
    "    std = (2 / n) ** 0.5\n",
    "\n",
    "    # init kernel params ~ Normal(0, std^2)\n",
    "    nn.init.normal_(conv2d_layer.weight, mean=0.0, std=std)\n",
    "\n",
    "    # init bias = 0\n",
    "    nn.init.zeros_(conv2d_layer.bias)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,num_classes=NUM_CLASSES):\n",
    "        super(Net, self).__init__()\n",
    "        # mtzk: p=0.005 is too small\n",
    "        #self.embedding_dropout = nn.Dropout(p = 0.005)\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        #self.conv1 = nn.Conv2d(3,64,3)\n",
    "        self.conv1 = nn.Conv2d(1,64,3) #single-channel\n",
    "        # nn.init.normal_(self.conv1.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64,128,3)\n",
    "        # nn.init.normal_(self.conv2.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128,512,3)\n",
    "        # nn.init.normal_(self.conv3.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv3)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(512,512,3)\n",
    "        # nn.init.normal_(self.conv4.weight, mean=0.0, std=0.1)\n",
    "        he_init(self.conv4)\n",
    "\n",
    "        # mtzk: BatchNorm2d は学習パラメータがあるので層ごとに違うのを使うべき\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 5 * 5, 4096)\n",
    "        # mtzk: - std changed according to He et al. (2015)\n",
    "        #       - init bias = zero\n",
    "        # nn.init.normal_(self.fc1.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        self.bn_fc1 = nn.BatchNorm1d(num_features=4096)\n",
    "\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        # nn.init.normal_(self.fc2.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        self.bn_fc2 = nn.BatchNorm1d(num_features=4096)\n",
    "\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "        # nn.init.normal_(self.fc3.weight, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.fc3.weight, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        # mtzk: BatchNorm2d は学習パラメータがあるので層ごとに違うのを使うべき\n",
    "        # x = self.bn3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # mtzk: 2023-12-28: BN also in fc\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78b973-eb59-4f64-9ff4-6615c8ed8273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "\n",
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self, batch_size=10, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size \n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        net = Net()\n",
    "        self.backbone = Net(num_classes=NUM_CLASSES)\n",
    "        \n",
    "        hidden_dim = NUM_CLASSES#net.fc1.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=2048,\n",
    "            output_dim=128,\n",
    "            num_layers=2,\n",
    "            batch_norm=True\n",
    "        )\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        #print(f\"Flattened output size: {h.size()}\")\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "#    def training_step(self, batch, batch_idx):\n",
    "        #print(f\"Batch content: {batch}\")  # バッチの内容を出力して確認\n",
    "#        (x0, x1), *_ = batch\n",
    "#        z0 = self.forward(x0)\n",
    "#        z1 = self.forward(x1)\n",
    "#        loss = self.criterion(z0, z1)\n",
    "#        self.log(\"train_loss_ssl\", loss)\n",
    "#        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch  # 画像とラベルを分けて取得\n",
    "\n",
    "        # 複数個の異なるビューを作成\n",
    "        views = [self.transform(images) for _ in range(5)]\n",
    "\n",
    "        # 各ビューについて順伝播を行い、特徴量を計算\n",
    "        embeddings = [self.forward(view) for view in views]\n",
    "\n",
    "        # 損失を計算（例: 各ペアの特徴量間で損失を計算し、その平均をとる）\n",
    "        total_loss = 0\n",
    "        num_pairs = 0\n",
    "\n",
    "        # すべてのペアの組み合わせで損失を計算\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i + 1, len(embeddings)):\n",
    "                total_loss += self.criterion(embeddings[i], embeddings[j])\n",
    "                num_pairs += 1\n",
    "\n",
    "        # ペア間の平均損失を計算\n",
    "        loss = total_loss / num_pairs\n",
    "\n",
    "        # ログに損失を記録\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):# lr=0.075*(self.batch_size)**(1/2)\n",
    "        #optim = torch.optim.SGD(\n",
    "        #    self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        #)\n",
    "        optim = torch.optim.Adam(\n",
    "            self.parameters(), lr=1e-3, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optim, max_epochs\n",
    "        )\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946c763-382b-417b-bc9f-594f8c9e4f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# 学習済みSimCLRモデルのバックボーンをロード\n",
    "# (例: ResNet-18)\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, simclr_model, num_classes):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # SimCLRバックボーンから特徴抽出部分のみを取得\n",
    "        self.backbone = Net(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# SimCLR学習済みモデルをロード（例）\n",
    "#simclr_model = torch.load(\"./params/1119-myNet-aug5-10ep.pth\")  # 学習済みモデル\n",
    "simclr_model = torch.load('./params/1206-myNet-aug5-10ep.pth')\n",
    "feature_extractor = FeatureExtractor(simclr_model, NUM_CLASSES)\n",
    "feature_extractor.eval()  # 特徴抽出用なので評価モード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8dee02-7fdf-41dd-88e6-ccdec03dd8ad",
   "metadata": {},
   "source": [
    "## 分類タスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fd99d-9960-451f-a72c-039413c83337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# 特徴ベクトルとラベルを準備\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # tqdmでデータローダーをラップして進捗を表示\n",
    "    for images, target in tqdm(dataloader_train, desc=\"Extracting features\"):\n",
    "        embedding = feature_extractor(images)  # 特徴を抽出\n",
    "        features.append(embedding)\n",
    "        labels.append(target)\n",
    "\n",
    "features = torch.cat(features)\n",
    "labels = torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a7fe3-b2aa-4f7f-9219-d04169f9a5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 線形分類器を構築\n",
    "classifier = nn.Linear(features.size(1), len(dataset_train.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# 分類器を学習\n",
    "for epoch in tqdm(range(10), desc=\"Training Classification; Epoch\"):  # 10エポック学習\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = classifier(features)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d18104-db0d-4d99-a802-7dc031c1ad4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テストデータで評価\n",
    "# 特徴抽出\n",
    "test_features = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, target in dataloader_train:\n",
    "        embedding = feature_extractor(images)\n",
    "        test_features.append(embedding)\n",
    "        test_labels.append(target)\n",
    "\n",
    "test_features = torch.cat(test_features)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n",
    "# 分類器で予測\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = classifier(test_features).argmax(dim=1)\n",
    "\n",
    "# 精度計算\n",
    "accuracy = (predictions == test_labels).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e1a55-d791-4f4b-8c38-f256b22f5ae0",
   "metadata": {},
   "source": [
    "## 特徴空間マッピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad3bb4-4090-480d-b3ab-849ffaa975b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 特徴ベクトルを保存するリスト\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in dataloader_test:\n",
    "        # 特徴ベクトルを抽出\n",
    "        embeddings = feature_extractor(images).flatten(start_dim=1)\n",
    "        features.append(embeddings)\n",
    "        labels.append(targets)\n",
    "\n",
    "# 特徴とラベルを結合\n",
    "features = torch.cat(features)  # [N, D] (N: サンプル数, D: 特徴次元)\n",
    "labels = torch.cat(labels)  # [N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928acc2-4961-46bd-8751-3ae59eba60b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 特徴ベクトルを正規化\n",
    "features = F.normalize(features, dim=1)\n",
    "\n",
    "# コサイン類似度行列の計算\n",
    "cosine_similarity_matrix = torch.mm(features, features.T)  # [N, N]\n",
    "\n",
    "# 例: クラスごとの平均類似度を計算\n",
    "unique_labels = labels.unique()\n",
    "class_similarities = {}\n",
    "for label in unique_labels:\n",
    "    class_indices = (labels == label).nonzero(as_tuple=True)[0]\n",
    "    class_features = features[class_indices]\n",
    "    class_similarity = torch.mm(class_features, class_features.T).mean().item()\n",
    "    class_similarities[label.item()] = class_similarity\n",
    "\n",
    "print(\"Class-wise Average Cosine Similarities:\")\n",
    "for label, sim in class_similarities.items():\n",
    "    print(f\"Class {label}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ea27d-d57a-4e9c-8379-f91cfd4b01ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNEによる次元削減\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "features_2d = tsne.fit_transform(features.cpu().numpy())\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels.cpu().numpy(), cmap='tab10', alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Class Labels\")\n",
    "plt.title(\"t-SNE Visualization of Latent Space\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad79c5-16a1-4273-8a8d-5bd00edde5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(features[:10])  # 最初の10サンプルの特徴ベクトルを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e4974-b91c-41e0-9d1e-1b2cb47f1e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):  # 最初の5枚を可視化\n",
    "    img, label = dataset_test[i]\n",
    "    plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0371-49ed-4126-a3b5-488a2293785b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_test_no_processing = transforms.Compose([\n",
    "    transforms.Resize((64, 63), antialias=True),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 前処理なしで画像を表示\n",
    "for i in range(5):\n",
    "    img, label = dataset_test[i]\n",
    "    img_no_processing = transform_test_no_processing(Image.open(dataset_test.image_paths[i]).convert('RGB'))\n",
    "    plt.imshow(img_no_processing.squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d96ec4-00aa-446f-9f27-6a167876b41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "normalized_features = F.normalize(features, dim=1)\n",
    "print(normalized_features[:10])  # 正規化後の特徴ベクトルを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2d754-4207-4cd7-a34c-6278baf2f758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x4fee/0x4fee-1.png'\n",
    "image_path = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x9aa8/0x9aa8-0.png'\n",
    "image_path = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/train/0x4e08/0x4e08-3.png'\n",
    "#image_path = '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x4fee/0x4fee-0.png'\n",
    "try:\n",
    "    image = Image.open(image_path)#.convert('RGB')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image at {image_path}: {e}\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0991b-b942-40f1-9741-2397430c709f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_paths = [\n",
    "    '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x4fee/0x4fee-1.png',\n",
    "    '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x9aa8/0x9aa8-0.png',\n",
    "    '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/train/0x4e08/0x4e08-3.png',\n",
    "    '/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test/0x4fee/0x4fee-0.png',\n",
    "]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    try:\n",
    "        # 画像を開いてRGBに変換\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        print(f\"Image at {image_path} converted to mode: {image.mode}\")  # モードを出力\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image at {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210bba2-6159-4f23-9e56-e93a679b5e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha_channel = image.split()[3]\n",
    "print(f\"Alpha channel unique values: {set(alpha_channel.getdata())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65e1a0-a0ba-4cb1-9069-ca9050848c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_array = np.array(image)\n",
    "print(f\"Image array shape: {image_array.shape}\")\n",
    "print(f\"Image array unique values: {np.unique(image_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba040d49-9be4-4268-ad57-fdde08477bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ベースディレクトリ\n",
    "input_dir = \"/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test\"\n",
    "output_dir = \"/home/abababam1/HandwrittenTextAlign/PRMU/simclr/data/test1\"\n",
    "\n",
    "for base_dir in glob.glob(f'{input_dir}/*'):\n",
    "    subdir_name = os.path.basename(base_dir)\n",
    "    # subdir内のPNGファイルをリストアップ\n",
    "    png_files = [f for f in os.listdir(base_dir) if f.endswith('.png')]\n",
    "\n",
    "    # PNGファイルを新しいディレクトリに移動・リネーム\n",
    "    for file_name in png_files:\n",
    "        old_file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "        # 新しいディレクトリ名とパス\n",
    "        idx = re.search(r'-(\\d+)(?=\\..*$)', file_name).group(1)\n",
    "        new_subdir_name = f\"subdir-{idx}\"\n",
    "        new_subdir_path = os.path.join(output_dir, subdir_name, new_subdir_name)\n",
    "        os.makedirs(new_subdir_path, exist_ok=True)  # 新しいディレクトリを作成\n",
    "\n",
    "        # 新しいファイル名とパス\n",
    "        new_file_name = f\"{new_subdir_name}.png\"\n",
    "        new_file_path = os.path.join(new_subdir_path, new_file_name)\n",
    "\n",
    "        # ファイルを移動・リネーム\n",
    "        shutil.move(old_file_path, new_file_path)\n",
    "        print(f\"Moved: {old_file_path} -> {new_file_path}\")\n",
    "\n",
    "    print(\"構造の変更が完了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageRecognition",
   "language": "python",
   "name": "imagerecognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
